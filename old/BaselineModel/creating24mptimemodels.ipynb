{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-18T06:04:29.207732Z","iopub.status.busy":"2022-05-18T06:04:29.207260Z","iopub.status.idle":"2022-05-18T06:04:30.559099Z","shell.execute_reply":"2022-05-18T06:04:30.558265Z","shell.execute_reply.started":"2022-05-18T06:04:29.207644Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import pickle\n","import gensim\n","import matplotlib.pyplot as plt\n","from joblib import Parallel, delayed\n","import csv\n","from csv import reader\n","from scipy import spatial\n","import functools\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# ****Getting lemmas and metadata together****"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:33.346810Z","iopub.status.busy":"2022-05-18T06:04:33.346157Z","iopub.status.idle":"2022-05-18T06:04:52.132912Z","shell.execute_reply":"2022-05-18T06:04:52.131218Z","shell.execute_reply.started":"2022-05-18T06:04:33.346783Z"},"trusted":true},"outputs":[],"source":["%%time\n","# open file in read mode\n","with open('/kaggle/input/utf8tokenizedspeeches/TokenizedSpeeches_utf-8.csv', 'r') as read_obj:\n","#with open('TokenizedSpeeches_utf-8.csv', 'r') as read_obj:\n","\n","    lemmasList = []\n","    \n","    # pass the file object to reader() to get the reader object\n","    csv_reader = reader(read_obj)\n","    # Iterate over each row in the csv using reader object\n","    for row in csv_reader: \n","        lemmasList.append(row)\n","    print(len(lemmasList), 'Rows read')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:52.134456Z","iopub.status.busy":"2022-05-18T06:04:52.134259Z","iopub.status.idle":"2022-05-18T06:04:52.458895Z","shell.execute_reply":"2022-05-18T06:04:52.458104Z","shell.execute_reply.started":"2022-05-18T06:04:52.134429Z"},"trusted":true},"outputs":[],"source":["%%time\n","#create dataframe from the lemmas extracted from csv\n","dictOfLemmas = {'Lemmas': lemmasList}\n","lemmasDf = pd.DataFrame(dictOfLemmas)\n","lemmasDf"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:52.460227Z","iopub.status.busy":"2022-05-18T06:04:52.459990Z","iopub.status.idle":"2022-05-18T06:04:58.158070Z","shell.execute_reply":"2022-05-18T06:04:58.157619Z","shell.execute_reply.started":"2022-05-18T06:04:52.460172Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/hansard-speeches-lemmatized/hansard-speeches-post2010.pkl', 'rb') as f:\n","    df = pickle.load(f)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.160099Z","iopub.status.busy":"2022-05-18T06:04:58.159887Z","iopub.status.idle":"2022-05-18T06:04:58.391907Z","shell.execute_reply":"2022-05-18T06:04:58.391255Z","shell.execute_reply.started":"2022-05-18T06:04:58.160071Z"},"trusted":true},"outputs":[],"source":["#since index was missing values and didn't match with the lemmasDf index\n","df = df.reset_index(drop=True)\n","df = df.join(lemmasDf)\n","df['Lemmas']"]},{"cell_type":"markdown","metadata":{},"source":["# Dividing and training corpus before and after the Brexit referendum"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.393415Z","iopub.status.busy":"2022-05-18T06:04:58.393181Z","iopub.status.idle":"2022-05-18T06:04:58.670768Z","shell.execute_reply":"2022-05-18T06:04:58.669997Z","shell.execute_reply.started":"2022-05-18T06:04:58.393387Z"},"trusted":true},"outputs":[],"source":["# Split data based on the Brexit referendum event before and after period\n","eventDate = '2016-06-23 23:59:59'\n","df_t1 = df[df['date']<= eventDate]\n","df_t2 = df[df['date']> eventDate]"]},{"cell_type":"markdown","metadata":{},"source":["# Choosing intersecting vocabulary and Aligning models "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.672047Z","iopub.status.busy":"2022-05-18T06:04:58.671846Z","iopub.status.idle":"2022-05-18T06:04:58.686003Z","shell.execute_reply":"2022-05-18T06:04:58.685109Z","shell.execute_reply.started":"2022-05-18T06:04:58.672019Z"},"trusted":true},"outputs":[],"source":["def intersection_align_gensim(m1, m2, words=None):\n","    \"\"\"\n","    Intersect two gensim word2vec models, m1 and m2.\n","    Only the shared vocabulary between them is kept.\n","    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n","    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n","    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n","        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n","        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n","    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n","    \"\"\"\n","\n","    # Get the vocab for each model\n","    vocab_m1 = set(m1.wv.index_to_key)\n","    vocab_m2 = set(m2.wv.index_to_key)\n","\n","    # Find the common vocabulary\n","    common_vocab = vocab_m1 & vocab_m2\n","    if words: common_vocab &= set(words)\n","\n","    # If no alignment necessary because vocab is identical...\n","    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n","        return (m1,m2)\n","\n","    # Otherwise sort by frequency (summed for both)\n","    common_vocab = list(common_vocab)\n","    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n","    # print(len(common_vocab))\n","\n","    # Then for each model...\n","    for m in [m1, m2]:\n","        # Replace old syn0norm array with new one (with common vocab)\n","        indices = [m.wv.key_to_index[w] for w in common_vocab]\n","        old_arr = m.wv.vectors\n","        new_arr = np.array([old_arr[index] for index in indices])\n","        m.wv.vectors = new_arr\n","\n","        # Replace old vocab dictionary with new one (with common vocab)\n","        # and old index2word with new one\n","        new_key_to_index = {}\n","        new_index_to_key = []\n","        for new_index, key in enumerate(common_vocab):\n","            new_key_to_index[key] = new_index\n","            new_index_to_key.append(key)\n","        m.wv.key_to_index = new_key_to_index\n","        m.wv.index_to_key = new_index_to_key\n","        \n","        print(len(m.wv.key_to_index), len(m.wv.vectors))\n","        if(len(m.wv.key_to_index)==135):\n","            print('Common vocab is', common_vocab)\n","        \n","    return (m1,m2)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.687613Z","iopub.status.busy":"2022-05-18T06:04:58.687381Z","iopub.status.idle":"2022-05-18T06:04:58.706874Z","shell.execute_reply":"2022-05-18T06:04:58.705588Z","shell.execute_reply.started":"2022-05-18T06:04:58.687585Z"},"trusted":true},"outputs":[],"source":["# Function to align two spaces with orthogunal procrustes\n","def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n","    \"\"\"\n","    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n","    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n","    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n","        \n","    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n","    Then do the alignment on the other_embed model.\n","    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n","    Return other_embed.\n","    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n","    \"\"\"\n","\n","    # make sure vocabulary and indices are aligned\n","    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n","    \n","    in_base_embed.wv.fill_norms(force=True)\n","    in_other_embed.wv.fill_norms(force=True)\n","        \n","    # get the (normalized) embedding matrices\n","    base_vecs = in_base_embed.wv.get_normed_vectors()\n","    other_vecs = in_other_embed.wv.get_normed_vectors()\n","\n","    # just a matrix dot product with numpy\n","    m = other_vecs.T.dot(base_vecs) \n","    # SVD method from numpy\n","    u, _, v = np.linalg.svd(m)\n","    # another matrix operation\n","    ortho = u.dot(v) \n","    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n","    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n","    \n","    return other_embed"]},{"cell_type":"markdown","metadata":{},"source":["# Splitting speeches by MPs and time"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.708006Z","iopub.status.busy":"2022-05-18T06:04:58.707833Z","iopub.status.idle":"2022-05-18T06:04:58.723781Z","shell.execute_reply":"2022-05-18T06:04:58.723272Z","shell.execute_reply.started":"2022-05-18T06:04:58.707984Z"},"trusted":true},"outputs":[],"source":["'''%%time\n","# Lets only consider the MPs who have speeches in both T1 & T2 time periods, intersection between two DFs \n","\n","t1List = pd.unique(df_t1['mnis_id']).tolist()\n","t2List = pd.unique(df_t2['mnis_id']).tolist()\n","\n","intersectedList = list(set(t1List).intersection(t2List))\n","\n","len(intersectedList)\n","# 651 MPs to be considered\n","\n","# Now modify original df_t1 and df_t2 DFs to only contain speeches from the common MPs \n","\n","df_t1 = df_t1[df_t1['mnis_id'].isin(intersectedList)]\n","df_t2 = df_t2[df_t2['mnis_id'].isin(intersectedList)]\n","df_t2['Lemmas']'''"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.725096Z","iopub.status.busy":"2022-05-18T06:04:58.724541Z","iopub.status.idle":"2022-05-18T06:04:58.813506Z","shell.execute_reply":"2022-05-18T06:04:58.812898Z","shell.execute_reply.started":"2022-05-18T06:04:58.725059Z"},"trusted":true},"outputs":[],"source":["\n","t1List = pd.unique(df_t1['mnis_id']).tolist()\n","t2List = pd.unique(df_t2['mnis_id']).tolist()\n","totalMps = set(t1List+t2List)\n","intersectedList = list(set(t1List).intersection(t2List))\n","print(len(totalMps), len(intersectedList))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.816342Z","iopub.status.busy":"2022-05-18T06:04:58.815516Z","iopub.status.idle":"2022-05-18T06:04:58.825815Z","shell.execute_reply":"2022-05-18T06:04:58.824763Z","shell.execute_reply.started":"2022-05-18T06:04:58.816301Z"},"trusted":true},"outputs":[],"source":["%%time\n","# Original code to divide by MP and time\n","# This creates DFs with vocabs of each MP split per speech\n","\n","'''dictSpeechesByMp = {}\n","\n","for mpId in intersectedList:\n","    for dfTime in ['df_t1','df_t2']:\n","        dfName = dfTime + '_'+ mpId\n","        if(dfTime == 'df_t1'):\n","            dictSpeechesByMp[dfName]=df_t1[df_t1['mnis_id']==mpId]\n","        elif (dfTime == 'df_t2'):\n","            dictSpeechesByMp[dfName]=df_t2[df_t2['mnis_id']==mpId]\n","            '''"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:04:58.828109Z","iopub.status.busy":"2022-05-18T06:04:58.827044Z","iopub.status.idle":"2022-05-18T06:05:45.689657Z","shell.execute_reply":"2022-05-18T06:05:45.689253Z","shell.execute_reply.started":"2022-05-18T06:04:58.828075Z"},"trusted":true},"outputs":[],"source":["%%time\n","# New code for dividing corpus by time and MPs \n","\n"," #   -x  -x  -x  Integrates all lemmas to map per MP - one vocab per MP per time period (eventually two vocabs per MP across T1 & T2)  -x  -x  -x  -x  -x\n","\n","dictSpeechesByMp = {}\n","\n","for mpId in totalMps:\n","\n","    for dfTime in ['df_t1','df_t2']:\n","\n","        tempDf = pd.DataFrame()\n","        tempList = []\n","        Lemmas =[]\n","        dfName = dfTime + '_'+ mpId\n","\n","        if(dfTime == 'df_t1'):\n","            tempDf = df_t1[df_t1['mnis_id']==mpId]\n","            \n","        elif (dfTime == 'df_t2'):\n","            tempDf = df_t2[df_t2['mnis_id']==mpId] \n","\n","        if (tempDf.shape[0]==0):\n","            continue\n","            \n","        tempList.extend(tempDf['Lemmas'].to_list())\n","        party = tempDf['party'].iat[0]\n","    \n","        #Flatten the list so it's not a list of lists\n","        tempList = [item for sublist in tempList for item in sublist]\n","        \n","        tempDf = pd.DataFrame([[mpId, party, tempList]],columns=['mnis_id', 'party', 'Lemmas'])\n","        dictSpeechesByMp[dfName]= tempDf\n","        dictSpeechesByMp[dfName]['df_name'] = dfName\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:05:45.690716Z","iopub.status.busy":"2022-05-18T06:05:45.690490Z","iopub.status.idle":"2022-05-18T06:05:45.695019Z","shell.execute_reply":"2022-05-18T06:05:45.694195Z","shell.execute_reply.started":"2022-05-18T06:05:45.690695Z"},"trusted":true},"outputs":[],"source":["len(dictSpeechesByMp.keys())"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T06:05:45.696109Z","iopub.status.busy":"2022-05-18T06:05:45.695920Z","iopub.status.idle":"2022-05-18T06:05:45.710602Z","shell.execute_reply":"2022-05-18T06:05:45.710044Z","shell.execute_reply.started":"2022-05-18T06:05:45.696082Z"},"trusted":true},"outputs":[],"source":["'''# Do not run, for verification only\n","\n","len(dictSpeechesByMp.keys()) \n","# Result 1302 \n","\n","df_t1['mnis_id'].value_counts()\n","df_t2['mnis_id'].value_counts()\n","# Result is 651 each - hence all covered in our dictionary DFs '''"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:12:21.006648Z","iopub.status.busy":"2022-05-18T07:12:21.006432Z","iopub.status.idle":"2022-05-18T07:12:21.865108Z","shell.execute_reply":"2022-05-18T07:12:21.864242Z","shell.execute_reply.started":"2022-05-18T07:12:21.006626Z"},"trusted":true},"outputs":[],"source":["%%time \n","mpTimeDf = pd.DataFrame(columns = ['mnis_id', 'party', 'Lemmas', 'df_name'])\n","for val in list(dictSpeechesByMp.values()):\n","    mpTimeDf = mpTimeDf.append(val)\n","mpTimeDf['LengthLemmas'] = mpTimeDf.Lemmas.map(len)\n","mpTimeDf.agg(Max=('LengthLemmas', max), Min=('LengthLemmas', 'min'), Mean=('LengthLemmas', np.mean))"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:12:22.462281Z","iopub.status.busy":"2022-05-18T07:12:22.461821Z","iopub.status.idle":"2022-05-18T07:12:22.467536Z","shell.execute_reply":"2022-05-18T07:12:22.466971Z","shell.execute_reply.started":"2022-05-18T07:12:22.462250Z"},"trusted":true},"outputs":[],"source":["mpTimeDf.shape"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:12:23.801914Z","iopub.status.busy":"2022-05-18T07:12:23.801658Z","iopub.status.idle":"2022-05-18T07:12:42.862590Z","shell.execute_reply":"2022-05-18T07:12:42.861595Z","shell.execute_reply.started":"2022-05-18T07:12:23.801885Z"},"trusted":true},"outputs":[],"source":["mpTimeDf = mpTimeDf[mpTimeDf['LengthLemmas']>10000] \n","#mpTimeDf['lemmas_delist'] = [','.join(map(str, l)) for l in mpTimeDf['Lemmas']]\n","\n","mpTimeDf['lemmas_delist'] = [','.join(map(str, l)) for l in mpTimeDf['Lemmas']]\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bbrexit\\b')]\n","print('after brexit', mpTimeDf.shape)\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bleave\\b')]\n","\n","print('after brexit and leave', mpTimeDf.shape)\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bleft\\b')]\n","print(mpTimeDf.shape, 'left') #(82, 9) \n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bremain\\b')]\n","print(mpTimeDf.shape, 'remain') #(82 9) \n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bleaving\\b')]\n","print(mpTimeDf.shape)\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bagreement\\b')]\n","print(mpTimeDf.shape)\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bforeign\\b')]\n","print(mpTimeDf.shape)\n","####\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bhard\\b')]\n","print(mpTimeDf.shape, 'hard')\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bline\\b')]\n","print(mpTimeDf.shape, 'line')\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bcontrol\\b')]\n","print(mpTimeDf.shape, 'cont')\n","\n","'''mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\binvoke\\b')]\n","print(mpTimeDf.shape,'invoke')\n","'''\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\breferendum\\b')]\n","print(mpTimeDf.shape, 'refer') #80 referendum\n","#newone\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bwar\\b')]\n","print(mpTimeDf.shape, 'war') \n","\n","####\n","\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bwithdrawal\\b')]\n","print(mpTimeDf.shape, 'withdrawal')\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bready\\b')]\n","print(mpTimeDf.shape, 'read') #81\n","#newone\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bexit\\b')]\n","print(mpTimeDf.shape, 'exit') #(69, 9) exit\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bedge\\b')]\n","print(mpTimeDf.shape, 'edge') #(57, 9) \n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bsoft\\b')]\n","print(mpTimeDf.shape, 'soft')\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\btrigger\\b')]\n","print(mpTimeDf.shape,'trigger')\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bwithdrawn\\b')]\n","print(mpTimeDf.shape, 'withdrawn')\n","\n","'''\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bdeparture\\b')]\n","print(mpTimeDf.shape, 'dep') #(38 eventuallty'''\n","\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bdeal\\b')]\n","print(mpTimeDf.shape,'deal')\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bred\\b')]\n","print(mpTimeDf.shape,'red')\n","\n","\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bdeparture\\b')]\n","print(mpTimeDf.shape, 'dep') #(38 eventuallty\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bprotest\\b')]\n","print(mpTimeDf.shape,'protest')\n","\n","\n","\n","\n","'''mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bprorogued\\b')]\n","print(mpTimeDf.shape, 'prorogued')\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bbrexiteer\\b')]\n","print(mpTimeDf.shape, 'brexiteer')\n","#83\n","\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bprorogue\\b')]\n","print(mpTimeDf.shape, 'prorogue')\n","#63\n","'''\n","\n","'''\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bcake\\b')]\n","print(mpTimeDf.shape, 'cake')\n","\n","mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\beurosceptic\\b')]\n","print(mpTimeDf.shape, 'eurosceptic')\n","\n","\n","tt = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\boven\\b')]\n","print(tt.shape, 'oven') - 10\n","\n","#\n","tt = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bdepreciate\\b')]\n","print(tt.shape, 'depreciate') - 6\n","\n","\n","tt = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains(r'\\bunicorn\\b')]\n","print(tt.shape, 'unicorn') - 32\n","'''\n","\n","\n","'''after brexit (531, 6)\n","after brexit and leave (531, 6)\n","(531, 6)\n","(531, 6)\n","(531, 6)\n","(531, 6)\n","(515, 6)\n","(475, 6) war\n","(411, 6) withdrawal\n","(236, 6) soft\n","(131, 6) trigger\n","(82, 6) withdrawn\n","(15, 6) prorogued\n","(6, 6) brexiteer\n","(5, 6) prorogue\n","(5, 6)'''\n","\n","mpTimeDf.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.439763Z","iopub.status.idle":"2022-05-17T22:17:02.440231Z","shell.execute_reply":"2022-05-17T22:17:02.440003Z","shell.execute_reply.started":"2022-05-17T22:17:02.439978Z"},"trusted":true},"outputs":[],"source":["'''# Plotting vocab per MP per timeperiod distribution\n","\n","plt.hist(vocabDf['LengthLemmas'], bins = 50)\n","plt.show()'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.441520Z","iopub.status.idle":"2022-05-17T22:17:02.442445Z","shell.execute_reply":"2022-05-17T22:17:02.442195Z","shell.execute_reply.started":"2022-05-17T22:17:02.442164Z"},"trusted":true},"outputs":[],"source":["# Checking for cutoff for vocab size\n","# Choosing all vocabs which have more than 10000 words\n","\n","#vocabDf = vocabDf[vocabDf['LengthLemmas']>6000] \n","#- 921 rows\n","\n","#vocabDf[vocabDf['LengthLemmas']>1000] = 1287 rows\n","#vocabDf[vocabDf['LengthLemmas']>5000] - 1123 rows\n"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:13:42.151409Z","iopub.status.busy":"2022-05-18T07:13:42.151138Z","iopub.status.idle":"2022-05-18T07:13:42.159335Z","shell.execute_reply":"2022-05-18T07:13:42.158590Z","shell.execute_reply.started":"2022-05-18T07:13:42.151381Z"},"trusted":true},"outputs":[],"source":["change = ['exiting', 'seaborne', 'eurotunnel', 'withdrawal', 'departures', 'unicorn', 'remainers', 'exit', 'surrender',\n","          'departure', 'triggering', 'stockpiling', 'expulsion', 'blindfold', 'cliff', 'lighter', 'exits', 'triggered',\n","          'brexiteer', 'soft', 'plus', 'trigger', 'backroom', 'invoked', 'protesting', 'brexit', 'edge', 'canary', \n","          'unicorns', 'withdrawing', 'invoking', 'withdrawn', 'manor', 'brexiteers', 'fanatics', 'postponement', \n","          'currencies', 'currency', 'operability', 'operable', 'leavers', 'invoke', 'article', 'eurozone', 'clueless',\n","          'surrendered', 'cake', 'red', 'euroscepticism', 'prorogation', 'lining', 'gove', 'norway', 'deflationary',\n","          'moribund', 'eurosceptic', 'deutschmark', 'courting', 'deal', 'withdraw', 'dab', 'withdrawals', 'eurosceptics',\n","          'surrendering', 'aldous', 'lanarkshire', 'leaving', 'signifying', 'roofs', 'ceded', 'absentia', 'treachery',\n","          'dollar', 'canada', 'pragmatist', 'oven', 'ready', 'brexiters', 'control', 'capitulation', 'leave', 'referendum',\n","          'agreement', 'prorogue', 'smoothest', 'depreciate', 'managed', 'mutiny', 'overvalued', 'ideologues', 'foreign',\n","          'eec', 'war', 'prorogued', 'hannan', 'appease', 'pendolino', 'southbound', 'left', 'line', 'hard', 'bill']\n"," "]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:13:42.576859Z","iopub.status.busy":"2022-05-18T07:13:42.576637Z","iopub.status.idle":"2022-05-18T07:13:42.582362Z","shell.execute_reply":"2022-05-18T07:13:42.581562Z","shell.execute_reply.started":"2022-05-18T07:13:42.576837Z"},"trusted":true},"outputs":[],"source":["mpTimeDf = mpTimeDf.reset_index()"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:13:42.837427Z","iopub.status.busy":"2022-05-18T07:13:42.837195Z","iopub.status.idle":"2022-05-18T07:13:42.842679Z","shell.execute_reply":"2022-05-18T07:13:42.841951Z","shell.execute_reply.started":"2022-05-18T07:13:42.837405Z"},"trusted":true},"outputs":[],"source":["mpTimeDf.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.462844Z","iopub.status.idle":"2022-05-17T22:17:02.463548Z","shell.execute_reply":"2022-05-17T22:17:02.463313Z","shell.execute_reply.started":"2022-05-17T22:17:02.463283Z"},"trusted":true},"outputs":[],"source":["mpTimeDf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.465045Z","iopub.status.idle":"2022-05-17T22:17:02.465772Z","shell.execute_reply":"2022-05-17T22:17:02.465504Z","shell.execute_reply.started":"2022-05-17T22:17:02.465476Z"},"trusted":true},"outputs":[],"source":["#mpTimeDf = mpTimeDf[mpTimeDf['lemmas_delist'].str.contains('departure')]\n","#mpTimeDf.shape"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:13:52.531022Z","iopub.status.busy":"2022-05-18T07:13:52.530749Z","iopub.status.idle":"2022-05-18T07:13:52.633975Z","shell.execute_reply":"2022-05-18T07:13:52.633187Z","shell.execute_reply.started":"2022-05-18T07:13:52.530992Z"},"trusted":true},"outputs":[],"source":["%%time\n","overlappingWords = []\n","for ind in mpTimeDf.index:\n","    mpVocabInTime = mpTimeDf.at[ind, 'Lemmas']\n","    #print(mpVocabInTime)\n","    overlap = list(set(mpVocabInTime).intersection(change))\n","    overlappingWords.append(overlap)\n","\n","mpTimeDf['overlapping_words'] = overlappingWords\n","mpTimeDf['overlapCount'] = mpTimeDf.overlapping_words.map(len)"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:13:53.135809Z","iopub.status.busy":"2022-05-18T07:13:53.135042Z","iopub.status.idle":"2022-05-18T07:13:53.144588Z","shell.execute_reply":"2022-05-18T07:13:53.143751Z","shell.execute_reply.started":"2022-05-18T07:13:53.135781Z"},"trusted":true},"outputs":[],"source":["mpTimeDf['overlapCount'].describe()"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:14:01.954228Z","iopub.status.busy":"2022-05-18T07:14:01.953373Z","iopub.status.idle":"2022-05-18T07:14:01.961911Z","shell.execute_reply":"2022-05-18T07:14:01.961260Z","shell.execute_reply.started":"2022-05-18T07:14:01.954142Z"},"trusted":true},"outputs":[],"source":["# Now reducing our changed words to include these AND also words that we DEFINITELY WANT\n","\n","listOfOverlapLists = mpTimeDf['overlapping_words'].to_list()\n","\n","# ******* CONFIRM WHY [0] ******\n","\n","x = set(listOfOverlapLists[0]).intersection(*listOfOverlapLists)\n","\n","#set([1])\n","#x.add('brexit') #58 from 132\n","#x.add('leave') #58 from 132\n","\n","#x.add('remain')\n","#x.add('exit') #60now\n","#x.add('cliff')\n","#x.add('trigger')\n","#x.add('withdraw')\n","#x.add('seaborne') #40 now\n","#x.add('remainers') #now 5 or 7 \n","\n","#x = list(x).append('brexit')\n","\n","# When vocab above 6000 considered, 'deal' is common\n","# When vocab above 20,000 considered, common vocab is {'deal', 'hard', 'leave', 'left'}\n","# Vocab above 40,000 considered , common words:\n","#{'agreement', 'bill', 'control', 'deal', 'hard', 'leave', 'left', 'line'}\n","# Vocab above 60,000, vocab - \n","#{'agreement', 'bill', 'control', 'deal', 'hard', 'leave', 'left', 'line'}\n","\n"," \n","x=list(x)\n","print(len(x),x)\n","#16 common when 82 models\n","# 24 common, 24 models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.473478Z","iopub.status.idle":"2022-05-17T22:17:02.473912Z","shell.execute_reply":"2022-05-17T22:17:02.473770Z","shell.execute_reply.started":"2022-05-17T22:17:02.473748Z"},"trusted":true},"outputs":[],"source":["'''overlappingWords2 = []\n","for ind in mpTimeDf.index:\n","    mpVocabInTime = mpTimeDf.at[ind, 'Lemmas']\n","    #print(mpVocabInTime)\n","    overlap = list(set(mpVocabInTime).intersection(x))\n","    overlappingWords2.append(overlap)\n","\n","mpTimeDf['overlapping_words2'] = overlappingWords2\n","mpTimeDf['overlapCount2'] = mpTimeDf.overlapping_words2.map(len)\n","mpTimeDf['overlapCount2'].describe()'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.474581Z","iopub.status.idle":"2022-05-17T22:17:02.475550Z","shell.execute_reply":"2022-05-17T22:17:02.475353Z","shell.execute_reply.started":"2022-05-17T22:17:02.475323Z"},"trusted":true},"outputs":[],"source":["#mpTimeDf['overlapping_words2'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.476535Z","iopub.status.idle":"2022-05-17T22:17:02.476892Z","shell.execute_reply":"2022-05-17T22:17:02.476744Z","shell.execute_reply.started":"2022-05-17T22:17:02.476711Z"},"trusted":true},"outputs":[],"source":["'''#mpTimeDf['overlap2_delist'] = str(mpTimeDf['overlapping_words2'])\n","mpTimeDf['overlap2_delist'] = [','.join(map(str, l)) for l in mpTimeDf['overlapping_words2']]\n","mpTimeDf_overidden = mpTimeDf[mpTimeDf['overlap2_delist'].str.contains('brexit')]\n","mpTimeDf_overidden = mpTimeDf[mpTimeDf['overlap2_delist'].str.contains('leave')]'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.477480Z","iopub.status.idle":"2022-05-17T22:17:02.477783Z","shell.execute_reply":"2022-05-17T22:17:02.477632Z","shell.execute_reply.started":"2022-05-17T22:17:02.477614Z"},"trusted":true},"outputs":[],"source":["mpTimeDf.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.478705Z","iopub.status.idle":"2022-05-17T22:17:02.478971Z","shell.execute_reply":"2022-05-17T22:17:02.478848Z","shell.execute_reply.started":"2022-05-17T22:17:02.478833Z"},"trusted":true},"outputs":[],"source":["mpTimeDf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.480435Z","iopub.status.idle":"2022-05-17T22:17:02.481002Z","shell.execute_reply":"2022-05-17T22:17:02.480856Z","shell.execute_reply.started":"2022-05-17T22:17:02.480837Z"},"trusted":true},"outputs":[],"source":["'''# We should only keep speeches by those MPs which have occurences in both T1 & T2 since comparison in their vocab is to be made here\n","\n","#list(vocabDf['mnis_id'].value_counts().value_counts())\n","#340 MPs have speeches in both T1 and T, 241 have speech(es) in one of the time periods\n","\n","mpCountDict = vocabDf['mnis_id'].value_counts().to_dict()\n","\n","vocabDf['mpCountInTimes'] = vocabDf['mnis_id'].map(mpCountDict)\n","\n","#Drop the MPs with selected vocab in only one time interval\n","\n","vocabDf=vocabDf[vocabDf['mpCountInTimes']==2]\n","\n","# For manual verification'''\n","'''vocabDf[vocabDf['mnis_id']=='3942']\n","3992, 3942 - only one instance of each\n","4263 - 2 instances'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.481958Z","iopub.status.idle":"2022-05-17T22:17:02.482388Z","shell.execute_reply":"2022-05-17T22:17:02.482254Z","shell.execute_reply.started":"2022-05-17T22:17:02.482236Z"},"trusted":true},"outputs":[],"source":["'''# Now backtrack and update dictionary to only keep DFs of these 921 rows\n","\n","# Creating dict key names to keep from our filtered DF mnis IDs\n","\n","mnis_id_list = list(vocabDf['mnis_id'].value_counts().to_dict().keys())\n","\n","list1 = ['df_t1_'+item for item in mnis_id_list]\n","list2 = ['df_t2_'+item for item in mnis_id_list]\n","mnis_id_list = list1 + list2'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.483140Z","iopub.status.idle":"2022-05-17T22:17:02.483652Z","shell.execute_reply":"2022-05-17T22:17:02.483497Z","shell.execute_reply.started":"2022-05-17T22:17:02.483478Z"},"trusted":true},"outputs":[],"source":["'''mnis_id_list = list(vocabDf['mnis_id'].value_counts().to_dict().keys())\n","len(mnis_id_list)'''"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:14:23.851480Z","iopub.status.busy":"2022-05-18T07:14:23.851186Z","iopub.status.idle":"2022-05-18T07:14:23.855720Z","shell.execute_reply":"2022-05-18T07:14:23.855143Z","shell.execute_reply.started":"2022-05-18T07:14:23.851454Z"},"trusted":true},"outputs":[],"source":["listDfsKeep = mpTimeDf['df_name'].to_list()"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:14:24.074738Z","iopub.status.busy":"2022-05-18T07:14:24.074300Z","iopub.status.idle":"2022-05-18T07:14:24.081797Z","shell.execute_reply":"2022-05-18T07:14:24.081106Z","shell.execute_reply.started":"2022-05-18T07:14:24.074706Z"},"trusted":true},"outputs":[],"source":["len(listDfsKeep)"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:14:26.676252Z","iopub.status.busy":"2022-05-18T07:14:26.675970Z","iopub.status.idle":"2022-05-18T07:14:26.881591Z","shell.execute_reply":"2022-05-18T07:14:26.880542Z","shell.execute_reply.started":"2022-05-18T07:14:26.676226Z"},"trusted":true},"outputs":[],"source":["# Dropping key-value pairs from dictionary where the key doesn't match\n","\n","for k,v in list(dictSpeechesByMp.items()):\n","    if (k not in listDfsKeep):\n","        del dictSpeechesByMp[k]"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:14:26.889964Z","iopub.status.busy":"2022-05-18T07:14:26.889136Z","iopub.status.idle":"2022-05-18T07:14:26.903042Z","shell.execute_reply":"2022-05-18T07:14:26.902001Z","shell.execute_reply.started":"2022-05-18T07:14:26.889929Z"},"trusted":true},"outputs":[],"source":["len(dictSpeechesByMp.values())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:14:36.145506Z","iopub.status.busy":"2022-05-18T07:14:36.145281Z","iopub.status.idle":"2022-05-18T07:14:55.353069Z","shell.execute_reply":"2022-05-18T07:14:55.351960Z","shell.execute_reply.started":"2022-05-18T07:14:36.145483Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","#  - x - x - x - CREATING & SAVING WORD2VEC MODELS FOR THE 680 MODELS - x - x - x \n","\n","dictOfModels = {}\n","'''\n","import shutil\n","shutil.rmtree('./models-by-mp-and-time')\n","'''\n","os.makedirs('./models-by-mp-and-time')\n","models_folder = './models-by-mp-and-time'\n","count = 1\n","\n","for dframe in dictSpeechesByMp: \n","    \n","# Doing in batches since notebook RAM crashe\n","    party = dictSpeechesByMp[dframe]['party'].values[0]\n","    if('&' in party):\n","        #print('............',party.split('& '))\n","        splitParty = party.split('&')\n","        party = ''\n","        for phr in splitParty:\n","            #print('dddd')\n","            party = party + str(phr)\n","        #print('11111111111111111',party)\n","    model = gensim.models.Word2Vec(dictSpeechesByMp[dframe]['Lemmas'], min_count=1, vector_size=300, window = 5, sg = 1)\n","\n","    # Also saving model in a dict and exporting\n","\n","    modelName ='model_'+ dframe+'_'+party\n","    print('model number', count, modelName)\n","\n","    dictOfModels[dframe] = model\n","    model.save(os.path.join(models_folder, modelName))\n","    count = count +1\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.492355Z","iopub.status.idle":"2022-05-17T22:17:02.492895Z","shell.execute_reply":"2022-05-17T22:17:02.492749Z","shell.execute_reply.started":"2022-05-17T22:17:02.492729Z"},"trusted":true},"outputs":[],"source":["'''%%time\n","# Commenting since we're going to load and start with alignment of models with a fresh notebook\n","functools.reduce(smart_procrustes_align_gensim, list(dictOfModels.values()))'''"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2022-05-18T07:15:09.974837Z","iopub.status.busy":"2022-05-18T07:15:09.974584Z","iopub.status.idle":"2022-05-18T07:15:36.192059Z","shell.execute_reply":"2022-05-18T07:15:36.191445Z","shell.execute_reply.started":"2022-05-18T07:15:09.974809Z"},"trusted":true},"outputs":[],"source":["%%time\n","!zip -r file.zip /kaggle/working/models-by-mp-and-time"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.496030Z","iopub.status.idle":"2022-05-17T22:17:02.496773Z","shell.execute_reply":"2022-05-17T22:17:02.496576Z","shell.execute_reply.started":"2022-05-17T22:17:02.496553Z"},"trusted":true},"outputs":[],"source":["# To delete if need be\n","'''import shutil\n","shutil.rmtree('./models-by-mp-and-time')\n","\n","os.remove('./file.zip')'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.497678Z","iopub.status.idle":"2022-05-17T22:17:02.498423Z","shell.execute_reply":"2022-05-17T22:17:02.498254Z","shell.execute_reply.started":"2022-05-17T22:17:02.498232Z"},"trusted":true},"outputs":[],"source":["dictOfModels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.499291Z","iopub.status.idle":"2022-05-17T22:17:02.499696Z","shell.execute_reply":"2022-05-17T22:17:02.499552Z","shell.execute_reply.started":"2022-05-17T22:17:02.499535Z"},"trusted":true},"outputs":[],"source":["%%time\n","modelsToAlign = list(dictOfModels.values())\n","for i in range(0,len(modelsToAlign)-1):\n","    functools.reduce(smart_procrustes_align_gensim, modelsToAlign)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.500959Z","iopub.status.idle":"2022-05-17T22:17:02.501474Z","shell.execute_reply":"2022-05-17T22:17:02.501338Z","shell.execute_reply.started":"2022-05-17T22:17:02.501320Z"},"trusted":true},"outputs":[],"source":["dictOfModels.keys()\n","dictOfModels['df_t2_1423'].wv.index_to_key"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.502645Z","iopub.status.idle":"2022-05-17T22:17:02.503394Z","shell.execute_reply":"2022-05-17T22:17:02.503223Z","shell.execute_reply.started":"2022-05-17T22:17:02.503202Z"},"trusted":true},"outputs":[],"source":["dictOfModels['df_t2_1423'].wv.vectors.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.504411Z","iopub.status.idle":"2022-05-17T22:17:02.505238Z","shell.execute_reply":"2022-05-17T22:17:02.505085Z","shell.execute_reply.started":"2022-05-17T22:17:02.505065Z"},"trusted":true},"outputs":[],"source":["t = 'brexit'\n","#dictOfModels['df_t2_1423'].wv.index_to_key\n","#dictOfModels['df_t2_1423'].layer1_size\n","\n","kis = ['df_t2_1423','df_t2_3945', 'df_t2_4070']\n","modelsSum = np.zeros(dictOfModels['df_t2_1423'].layer1_size)\n","\n","for k in kis:\n","    #Something like this to refer to t's vector\n","    #vector = dictOfModels[k].wv.get_index(t)#\n","    vector = dictOfModels[k].wv[t]\n","    modelsSum = np.add(modelsSum, vector)\n","    \n","avgEmbedding =np.divide(modelsSum, len(kis))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.506000Z","iopub.status.idle":"2022-05-17T22:17:02.506754Z","shell.execute_reply":"2022-05-17T22:17:02.506565Z","shell.execute_reply.started":"2022-05-17T22:17:02.506543Z"},"trusted":true},"outputs":[],"source":["avgEmbedding.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.508026Z","iopub.status.idle":"2022-05-17T22:17:02.508628Z","shell.execute_reply":"2022-05-17T22:17:02.508466Z","shell.execute_reply.started":"2022-05-17T22:17:02.508445Z"},"trusted":true},"outputs":[],"source":["dictOfModels['df_t2_1423'].wv.get_index('brexit')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.509677Z","iopub.status.idle":"2022-05-17T22:17:02.510074Z","shell.execute_reply":"2022-05-17T22:17:02.509862Z","shell.execute_reply.started":"2022-05-17T22:17:02.509846Z"},"trusted":true},"outputs":[],"source":["dictOfModels['df_t2_1423'].wv.get_item('brexit')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.511032Z","iopub.status.idle":"2022-05-17T22:17:02.511324Z","shell.execute_reply":"2022-05-17T22:17:02.511193Z","shell.execute_reply.started":"2022-05-17T22:17:02.511177Z"},"trusted":true},"outputs":[],"source":["def cosine_similarity(word):\n","  sc = 1-spatial.distance.cosine(dictOfModels['df_t2_3945'].wv[t], dictOfModels['df_t2_1423'].wv[t])\n","  return sc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.512488Z","iopub.status.idle":"2022-05-17T22:17:02.512780Z","shell.execute_reply":"2022-05-17T22:17:02.512632Z","shell.execute_reply.started":"2022-05-17T22:17:02.512617Z"},"trusted":true},"outputs":[],"source":["cosine_similarity('brexit')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.513586Z","iopub.status.idle":"2022-05-17T22:17:02.513880Z","shell.execute_reply":"2022-05-17T22:17:02.513754Z","shell.execute_reply.started":"2022-05-17T22:17:02.513739Z"},"trusted":true},"outputs":[],"source":["dictOfModels['df_t2_1423'].wv['brexit']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.514713Z","iopub.status.idle":"2022-05-17T22:17:02.514981Z","shell.execute_reply":"2022-05-17T22:17:02.514856Z","shell.execute_reply.started":"2022-05-17T22:17:02.514841Z"},"trusted":true},"outputs":[],"source":["avgEmbedding.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.516312Z","iopub.status.idle":"2022-05-17T22:17:02.516904Z","shell.execute_reply":"2022-05-17T22:17:02.516762Z","shell.execute_reply.started":"2022-05-17T22:17:02.516743Z"},"trusted":true},"outputs":[],"source":["\n","'''def pattern2vector(tokens, word2vec, AVG=False):\n","    pattern_vector = np.zeros(word2vec.layer1_size)\n","    n_words = 0\n","    if len(tokens) > 1:\n","        for t in tokens:\n","            try:\n","                vector = word2vec[t.strip()]\n","                pattern_vector = np.add(pattern_vector,vector)\n","                n_words += 1\n","            except KeyError, e:\n","                continue\n","        if AVG is True:\n","            pattern_vector = np.divide(pattern_vector,n_words)\n","    elif len(tokens) == 1:\n","        try:\n","            pattern_vector = word2vec[tokens[0].strip()]\n","        except KeyError:\n","            pass\n","    return pattern_vector\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.518294Z","iopub.status.idle":"2022-05-17T22:17:02.518997Z","shell.execute_reply":"2022-05-17T22:17:02.518833Z","shell.execute_reply.started":"2022-05-17T22:17:02.518811Z"},"trusted":true},"outputs":[],"source":["#np.mean(dictOfModels['df_t2_1423'][['brexit']],axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.519903Z","iopub.status.idle":"2022-05-17T22:17:02.520442Z","shell.execute_reply":"2022-05-17T22:17:02.520301Z","shell.execute_reply.started":"2022-05-17T22:17:02.520283Z"},"trusted":true},"outputs":[],"source":["'df_t2_3945', 'df_t2_4070'\n","def pattern2vector(tokens, word2vec):\n","    pattern_vector = np.zeros(word2vec.layer1_size)\n","    n_words = 0\n","    if len(tokens) > 1:\n","        for t in tokens:\n","            try:\n","                vector = word2vec[t.strip()]\n","                pattern_vector = np.add(pattern_vector,vector)\n","                n_words += 1\n","            except KeyError, e:\n","                continue\n","        if AVG is True:\n","            pattern_vector = np.divide(pattern_vector,n_words)\n","    elif len(tokens) == 1:\n","        try:\n","            pattern_vector = word2vec[tokens[0].strip()]\n","        except KeyError:\n","            pass\n","    return pattern_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.521138Z","iopub.status.idle":"2022-05-17T22:17:02.521680Z","shell.execute_reply":"2022-05-17T22:17:02.521542Z","shell.execute_reply.started":"2022-05-17T22:17:02.521524Z"},"trusted":true},"outputs":[],"source":["def get_mean_vector(word2vec_models):\n","    # remove out-of-vocabulary words\n","    #words = [word for word in words if word in word2vec_model.vocab]\n","    if len(words) >= 1:\n","        return np.mean(word2vec_model[words], axis=0)\n","    else:\n","        return []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.522355Z","iopub.status.idle":"2022-05-17T22:17:02.522922Z","shell.execute_reply":"2022-05-17T22:17:02.522776Z","shell.execute_reply.started":"2022-05-17T22:17:02.522755Z"},"trusted":true},"outputs":[],"source":["dictOfModels\n","for i in dictOfModels.keys():\n","  print(dictOfModels[i], dictOfModels[i].wv.similar_by_word('brexit', 10))\n","  print('- x - x - x - x - x - x - x - x - x')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.523587Z","iopub.status.idle":"2022-05-17T22:17:02.524230Z","shell.execute_reply":"2022-05-17T22:17:02.524086Z","shell.execute_reply.started":"2022-05-17T22:17:02.524068Z"},"trusted":true},"outputs":[],"source":["dictOfModels.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.524956Z","iopub.status.idle":"2022-05-17T22:17:02.525659Z","shell.execute_reply":"2022-05-17T22:17:02.525490Z","shell.execute_reply.started":"2022-05-17T22:17:02.525469Z"},"trusted":true},"outputs":[],"source":["set(dictOfModels['df_t1_1427'].wv.index_to_key).intersection(x)\n","# Common vocab of 1071, 10 common words with our words"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.526627Z","iopub.status.idle":"2022-05-17T22:17:02.527329Z","shell.execute_reply":"2022-05-17T22:17:02.527167Z","shell.execute_reply.started":"2022-05-17T22:17:02.527145Z"},"trusted":true},"outputs":[],"source":["def cosine_similarity(word):\n","  sc = 1-spatial.distance.cosine(model1.wv[word], model2.wv[word])\n","  return sc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-17T22:17:02.528576Z","iopub.status.idle":"2022-05-17T22:17:02.529424Z","shell.execute_reply":"2022-05-17T22:17:02.529260Z","shell.execute_reply.started":"2022-05-17T22:17:02.529237Z"},"trusted":true},"outputs":[],"source":["%%time\n","cosine_similarity_df = pd.DataFrame(([w, cosine_similarity(w), model1.wv.get_vecattr(w, \"count\") , model2.wv.get_vecattr(w, \"count\") ] for w in model1.wv.index_to_key), columns = ('Word', 'Cosine_similarity', \"Frequency_t1\", \"Frequency_t2\"))\n","\n","cosine_similarity_df['FrequencyRatio'] = cosine_similarity_df['Frequency_t1']/cosine_similarity_df['Frequency_t2']\n","cosine_similarity_df['TotalFrequency'] = cosine_similarity_df['Frequency_t1'] + cosine_similarity_df['Frequency_t2']\n","\n","cosine_similarity_df_sorted = cosine_similarity_df.sort_values(by='Cosine_similarity', ascending=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
