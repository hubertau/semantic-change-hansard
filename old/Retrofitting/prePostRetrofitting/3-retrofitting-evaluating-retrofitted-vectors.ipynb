{"cells":[{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:06.113211Z","iopub.status.busy":"2022-06-21T16:59:06.112764Z","iopub.status.idle":"2022-06-21T16:59:17.015759Z","shell.execute_reply":"2022-06-21T16:59:17.014760Z","shell.execute_reply.started":"2022-06-21T16:59:06.113176Z"},"trusted":true},"outputs":[],"source":["!pip install researchpy\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import pickle\n","import gensim\n","import matplotlib.pyplot as plt\n","from joblib import Parallel, delayed\n","import csv\n","from csv import reader\n","from scipy import spatial\n","import functools\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","\n","import imblearn\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","import researchpy as rp\n","import scipy.stats as stats\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session~"]},{"cell_type":"markdown","metadata":{},"source":["# **Importing & Processing Retrofitted vectors**"]},{"cell_type":"markdown","metadata":{},"source":["**Reading retrofitted out-vector file**"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:17.017656Z","iopub.status.busy":"2022-06-21T16:59:17.017328Z","iopub.status.idle":"2022-06-21T16:59:17.259074Z","shell.execute_reply":"2022-06-21T16:59:17.258103Z","shell.execute_reply.started":"2022-06-21T16:59:17.017623Z"},"trusted":true},"outputs":[],"source":["%%time\n","retrofittedVectorPath = '/kaggle/input/retrofittedvectors/out_vec_file.txt'\n","retrofittingFactor = 'party'\n","\n","#retrofittedVectorPath = '/kaggle/input/retrofittedpartytimevectors/retrofittedPartyTimeVectors.txt'\n","#retrofittingFactor = 'party-time'\n","\n","with open(retrofittedVectorPath) as f:\n","\n","    vecs=[]\n","    vec=''\n","\n","    while True:\n","        line = f.readline()\n","        if not line: \n","            break        \n","        if(str(list(line)[0]).isalpha()):\n","            vec=vec.strip()\n","            if(vec!=''):\n","                vecs.append(vec)\n","            vec = line\n","        else:\n","            vec+=line\n","        \n","vecs = [vec.replace('\\n', '')for vec in vecs]\n","print(str(len(vecs))+' Retrofitted vectors obtained')"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:17.261339Z","iopub.status.busy":"2022-06-21T16:59:17.260690Z","iopub.status.idle":"2022-06-21T16:59:17.276603Z","shell.execute_reply":"2022-06-21T16:59:17.275567Z","shell.execute_reply.started":"2022-06-21T16:59:17.261291Z"},"trusted":true},"outputs":[],"source":["change = ['exiting', 'seaborne', 'eurotunnel', 'withdrawal', 'departures', 'unicorn', 'remainers', 'exit', 'surrender',\n","          'departure', 'triggering', 'stockpiling', 'expulsion', 'blindfold', 'cliff', 'lighter', 'exits', 'triggered',\n","          'brexiteer', 'soft', 'plus', 'trigger', 'backroom', 'invoked', 'protesting', 'brexit', 'edge', 'canary', \n","          'unicorns', 'withdrawing', 'invoking', 'withdrawn', 'manor', 'brexiteers', 'fanatics', 'postponement', \n","          'currencies', 'currency', 'operability', 'operable', 'leavers', 'invoke', 'article', 'eurozone', 'clueless',\n","          'surrendered', 'cake', 'red', 'euroscepticism', 'prorogation', 'lining', 'gove', 'norway', 'deflationary',\n","          'moribund', 'eurosceptic', 'deutschmark', 'courting', 'deal', 'withdraw', 'dab', 'withdrawals', 'eurosceptics',\n","          'surrendering', 'aldous', 'lanarkshire', 'leaving', 'signifying', 'roofs', 'ceded', 'absentia', 'treachery',\n","          'dollar', 'canada', 'pragmatist', 'oven', 'ready', 'brexiters', 'control', 'capitulation', 'leave', 'referendum',\n","          'agreement', 'prorogue', 'smoothest', 'depreciate', 'managed', 'mutiny', 'overvalued', 'ideologues', 'foreign',\n","          'eec', 'war', 'prorogued', 'hannan', 'appease', 'pendolino', 'southbound', 'left', 'line', 'hard', 'bill']\n"," \n","no_change = ['prime', 'even', 'parliament', 'care', 'well', 'constituency', 'tax', 'children',\n","             'business', 'report', 'case', 'sure', 'like', 'see', 'state', 'order', 'back', 'new', 'hope', 'local',\n","             'secretary', 'public', 'right', 'much', 'say', 'first', 'minister', 'look', 'system', 'whether', \n","             'members', 'million', 'good', 'today', 'services', 'clear', 'help', 'time', 'place', 'put', 'last', 'must', 'money', 'one', \n","             'way', 'work', 'would', 'think', 'two', 'great', 'could', 'lady', 'us', 'come', 'however', 'may', 'going', 'go',\n","             'given', 'year', 'might', 'part', 'get', 'make', 'point', 'committee', 'years', 'also', 'know',\n","             'government', 'take', 'house', 'agree', 'member', 'number', 'across', 'made', 'give', 'gentleman', 'important', 'said',\n","             'people', 'issue', 'support', 'ensure']\n","\n","words_of_interest= change+no_change"]},{"cell_type":"markdown","metadata":{},"source":["**Extracting vectors & mapping to synonym key, checking for dimensions**"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:17.873654Z","iopub.status.busy":"2022-06-21T16:59:17.872916Z","iopub.status.idle":"2022-06-21T16:59:18.185188Z","shell.execute_reply":"2022-06-21T16:59:18.184289Z","shell.execute_reply.started":"2022-06-21T16:59:17.873603Z"},"trusted":true},"outputs":[],"source":["dictKeyVector = {}\n","count=0\n","for i in range(len(vecs)):\n","    \n","    vec = vecs[i].strip().split(' ')\n","    # Extracting synonym key\n","    synKey = vec[0]\n","    del(vec[0])\n","    vec=[i for i in vec if i!='']\n","    \n","    if(len(vec)!=300):\n","        print('Vector with dimension<300', synKey,len(vec))\n","        count=count+1\n","    else:\n","        vec =[float(v) for v in vec]\n","        dictKeyVector[synKey]=vec\n","        npVec = np.array(dictKeyVector[synKey])\n","print('Count of vectors with fewer dimensions that we will not consider',count)\n","dfRetrofitted = pd.DataFrame({'vectorKey':list(dictKeyVector.keys()), 'vectors':list(dictKeyVector.values())})\n","dfRetrofitted.head()\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:18.800810Z","iopub.status.busy":"2022-06-21T16:59:18.800344Z","iopub.status.idle":"2022-06-21T16:59:18.808339Z","shell.execute_reply":"2022-06-21T16:59:18.807148Z","shell.execute_reply.started":"2022-06-21T16:59:18.800773Z"},"trusted":true},"outputs":[],"source":["dfRetrofitted.shape"]},{"cell_type":"markdown","metadata":{},"source":["**For party based retrofitting\n","2071 retrofitted vectors were expected. \n","2070 were created. \n","55 vectors discarded that had dimensions<300\n","2015 vectors left** \n","\n","\n","**For party-time retrofitting - From 1962 input vectors, 1634 retrofitted vectors were received(328 were lost during retrofitting, no reason found yet), \n","Further, 35 vectors have been discarded as the vector dimensions were lost (under 300)\n","Eventually left with 1599**"]},{"cell_type":"markdown","metadata":{},"source":["# **Calculating Cosine similarity**"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:23.602387Z","iopub.status.busy":"2022-06-21T16:59:23.601858Z","iopub.status.idle":"2022-06-21T16:59:23.610292Z","shell.execute_reply":"2022-06-21T16:59:23.609458Z","shell.execute_reply.started":"2022-06-21T16:59:23.602355Z"},"trusted":true},"outputs":[],"source":["# Filtering down words of interest as per those present in our vectors \n","# We're amending the computeAvgVec function accordingly\n","# As it calculated based on processing from models, and here we're only taking vectors. Hence this check here too.\n","\n","vectorKeys =list(dfRetrofitted['vectorKey'])\n","# Extracting words from vectors keys\n","words_of_interest = list(set([vk.split('-')[0] for vk in vectorKeys]))\n","print(words_of_interest, len(words_of_interest))\n","\n","# NOW WE ONLY HAVE THOSE WORDS HERE WHICH ARE PRESENT IN THE VECTORS. "]},{"cell_type":"markdown","metadata":{},"source":["**Functions for cosine similarity computation and to compute the average vector amongst many vectors for a given word**"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:25.482693Z","iopub.status.busy":"2022-06-21T16:59:25.482142Z","iopub.status.idle":"2022-06-21T16:59:25.491724Z","shell.execute_reply":"2022-06-21T16:59:25.490672Z","shell.execute_reply.started":"2022-06-21T16:59:25.482661Z"},"trusted":true},"outputs":[],"source":["# Different from the avg computation function in our other scripts. This works upon vectors instead of models\n","def computeAvgVec(mKeys, dicto, w, layerSize=300):\n","    modelsSum = np.zeros(layerSize)\n","    for k in mKeys:\n","        vectorPerModel = dicto[k]\n","        modelsSum = np.add(modelsSum, vectorPerModel)\n","    avgEmbedding =np.divide(modelsSum, len(mKeys))\n","    return avgEmbedding\n","\n","def cosine_similarity(vec1, vec2):\n","  sc = 1-spatial.distance.cosine(vec1, vec2)\n","  return sc\n","\n","cosine_similarity_df = pd.DataFrame(columns = ('Word', 'Cosine_similarity'))"]},{"cell_type":"markdown","metadata":{},"source":["**Compute cosine similarity between avg T1 and T2 vectors**"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:26.992523Z","iopub.status.busy":"2022-06-21T16:59:26.992102Z","iopub.status.idle":"2022-06-21T16:59:27.175849Z","shell.execute_reply":"2022-06-21T16:59:27.174681Z","shell.execute_reply.started":"2022-06-21T16:59:26.992490Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","t1Keys = [t for t in list(dictKeyVector.keys()) if 't1' in t]\n","t2Keys = [t for t in list(dictKeyVector.keys()) if 't2' in t]\n","sims= []\n","\n","# Compute average of word in T1 and in T2 and store average vectors and cosine difference   \n","for word in words_of_interest:\n","        \n","        #Provide a list of keys to average computation model for it to\n","        #compute average vector amongst these models\n","        wordT1Keys = [k for k in t1Keys if k.split('-')[0]==word]\n","        wordT2Keys = [k for k in t2Keys if k.split('-')[0]==word]\n","        \n","        #Since here the key itself contains the word we're not simply sending T1 keys but sending word-wise key\n","        avgVecT1 = computeAvgVec(wordT1Keys, dictKeyVector, word)\n","        avgVecT2 = computeAvgVec(wordT2Keys, dictKeyVector, word)\n","        \n","        if(avgVecT1.shape == avgVecT2.shape):\n","            # Cos similarity between averages\n","            cosSimilarity = cosine_similarity(avgVecT1, avgVecT2)\n","            sims.append(cosSimilarity)\n","        else:\n","            print('Word not found')\n","cosine_similarity_df['Word']=words_of_interest\n","cosine_similarity_df['Cosine_similarity']=sims\n","\n","'''\n","cosine_similarity_df_sorted = cosine_similarity_df.sort_values(by='Cosine_similarity', ascending=True)\n","cosine_similarity_df_sorted'''\n","\n","#Assigning change and no-change labels as initially expected\n","cosine_similarity_df['semanticDifference']=['default' for i in range(cosine_similarity_df.shape[0])]\n","cosine_similarity_df.loc[cosine_similarity_df['Word'].isin(change), 'semanticDifference'] = 'change' \n","cosine_similarity_df.loc[cosine_similarity_df['Word'].isin(no_change), 'semanticDifference'] = 'no_change' \n","cosine_similarity_df.sort_values(by='Cosine_similarity').head(10)"]},{"cell_type":"markdown","metadata":{},"source":["# **LOGISTIC REGRESSION**"]},{"cell_type":"markdown","metadata":{},"source":["**Evaluate the performance of retrofitted vectors**"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:28.522549Z","iopub.status.busy":"2022-06-21T16:59:28.522125Z","iopub.status.idle":"2022-06-21T16:59:28.527398Z","shell.execute_reply":"2022-06-21T16:59:28.526162Z","shell.execute_reply.started":"2022-06-21T16:59:28.522515Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:29.547181Z","iopub.status.busy":"2022-06-21T16:59:29.546787Z","iopub.status.idle":"2022-06-21T16:59:29.570597Z","shell.execute_reply":"2022-06-21T16:59:29.569344Z","shell.execute_reply.started":"2022-06-21T16:59:29.547147Z"},"trusted":true},"outputs":[],"source":["X = cosine_similarity_df['Cosine_similarity'].values.reshape(-1,1)\n","y = cosine_similarity_df['semanticDifference']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n","\n","undersample = RandomUnderSampler(sampling_strategy=1.0)\n","\n","X_over, y_over = undersample.fit_resample(X, y)\n","X=X_over\n","y=y_over\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n","\n","logreg = LogisticRegression()\n","kf = logreg.fit(X_train, y_train)\n","\n","y_pred = logreg.predict(X_test)\n","\n","print('Y value counts',y.value_counts(),'\\n')\n","print('Y train value counts', y_train.value_counts())"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:30.662439Z","iopub.status.busy":"2022-06-21T16:59:30.662005Z","iopub.status.idle":"2022-06-21T16:59:30.753154Z","shell.execute_reply":"2022-06-21T16:59:30.751997Z","shell.execute_reply.started":"2022-06-21T16:59:30.662408Z"},"trusted":true},"outputs":[],"source":["#scoring = {}\n","print(accuracy_score)\n","#if (retrofittingFactor=='party'):\n","    \n","scoring = {'accuracy' : make_scorer(accuracy_score), \n","               'precision' : make_scorer(precision_score,pos_label='change'),\n","               'recall' : make_scorer(recall_score,pos_label='change'), \n","               'f1_score' : make_scorer(f1_score,pos_label='change')}\n","\n","scores = cross_validate(kf, X, y, cv=10, scoring=scoring,error_score='raise')\n","\n","print('Accuracy', scores['test_accuracy'].mean())\n","print('Precision', scores['test_precision'].mean())\n","print('Recall', scores['test_recall'].mean())\n","print('F1 Score', scores['test_f1_score'].mean())\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:31.962891Z","iopub.status.busy":"2022-06-21T16:59:31.962483Z","iopub.status.idle":"2022-06-21T16:59:32.219626Z","shell.execute_reply":"2022-06-21T16:59:32.218542Z","shell.execute_reply.started":"2022-06-21T16:59:31.962858Z"},"trusted":true},"outputs":[],"source":["cf_matrix = confusion_matrix(y_test, y_pred)\n","\n","ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n","\n","ax.set_title('Confusion Matrix for vectors retrofitted on the basis of same party \\n\\n');\n","ax.set_xlabel('\\nPredicted Values')\n","ax.set_ylabel('Actual Values ');\n","\n","ax.xaxis.set_ticklabels(['change','no_change'])\n","ax.yaxis.set_ticklabels(['change','no_change'])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:33.514978Z","iopub.status.busy":"2022-06-21T16:59:33.514511Z","iopub.status.idle":"2022-06-21T16:59:33.547014Z","shell.execute_reply":"2022-06-21T16:59:33.545757Z","shell.execute_reply.started":"2022-06-21T16:59:33.514942Z"},"trusted":true},"outputs":[],"source":["# T Test \n","\n","summary, results = rp.ttest(group1= cosine_similarity_df['Cosine_similarity'][cosine_similarity_df['semanticDifference'] == 'change'], group1_name= \"change\",\n","                            group2= cosine_similarity_df['Cosine_similarity'][cosine_similarity_df['semanticDifference'] == 'no_change'], group2_name= \"no_change\")\n","print(summary)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:35.528435Z","iopub.status.busy":"2022-06-21T16:59:35.527957Z","iopub.status.idle":"2022-06-21T16:59:35.534346Z","shell.execute_reply":"2022-06-21T16:59:35.533097Z","shell.execute_reply.started":"2022-06-21T16:59:35.528398Z"},"trusted":true},"outputs":[],"source":["accuracy, precision, recall, f1_score = [], [], [], []\n","retrofittingBasis = [retrofittingFactor]"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:39.305080Z","iopub.status.busy":"2022-06-21T16:59:39.304672Z","iopub.status.idle":"2022-06-21T16:59:39.312109Z","shell.execute_reply":"2022-06-21T16:59:39.310724Z","shell.execute_reply.started":"2022-06-21T16:59:39.305048Z"},"trusted":true},"outputs":[],"source":["#scoresDf = pd.DataFrame(columns= ['retrofittingBasis','Accuracy','Precision','Recall','F1Score'])\n","#scoresDf.append(['party', scores['test_accuracy'].mean(), scores['test_precision'].mean(),scores['test_recall'].mean(), scores['test_f1_score'].mean()],axis=1)\n","accuracy.append(scores['test_accuracy'].mean())\n","precision.append(scores['test_precision'].mean())\n","recall.append(scores['test_recall'].mean())\n","f1_score.append(scores['test_f1_score'].mean())"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:48:52.243101Z","iopub.status.busy":"2022-06-21T16:48:52.242668Z","iopub.status.idle":"2022-06-21T16:48:52.249256Z","shell.execute_reply":"2022-06-21T16:48:52.248299Z","shell.execute_reply.started":"2022-06-21T16:48:52.243067Z"},"trusted":true},"outputs":[],"source":["'''scoresDict = {'retrofittingBasis': 'party','Accuracy':[scores['test_accuracy'].mean()],'Precision':scores['test_precision'].mean(),'Recall':scores['test_recall'].mean(),'F1Score':scores['test_f1_score'].mean()}\n","scoresDf = pd.DataFrame(scoresDict)\n","\n","pd.concat([scoresDf,np.series(['party', scores['test_accuracy'].mean(), scores['test_precision'].mean(),scores['test_recall'].mean(), scores['test_f1_score'].mean()]) ])'''"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T16:59:50.923926Z","iopub.status.busy":"2022-06-21T16:59:50.923409Z","iopub.status.idle":"2022-06-21T16:59:50.943043Z","shell.execute_reply":"2022-06-21T16:59:50.941795Z","shell.execute_reply.started":"2022-06-21T16:59:50.923888Z"},"trusted":true},"outputs":[],"source":["scoresDict = {'retrofittingBasis': retrofittingBasis,'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1Score':f1_score}\n","if(retrofittingFactor=='party'):\n","    scoresDf = pd.DataFrame(scoresDict)\n","else:\n","    scoresDf = pd.concat([scoresDf, pd.DataFrame(scoresDict)])\n","scoresDf"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-06-21T17:00:03.255945Z","iopub.status.busy":"2022-06-21T17:00:03.255024Z","iopub.status.idle":"2022-06-21T17:00:03.260211Z","shell.execute_reply":"2022-06-21T17:00:03.259155Z","shell.execute_reply.started":"2022-06-21T17:00:03.255902Z"},"trusted":true},"outputs":[],"source":["#Just in case\n","#scoresDf.to_pickle('./partyScore.pkl')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
