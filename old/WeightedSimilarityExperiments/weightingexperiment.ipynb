{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nimport gensim\nimport matplotlib.pyplot as plt\nfrom joblib import Parallel, delayed\nimport csv\nfrom csv import reader\nfrom scipy import spatial\nimport functools\nfrom collections import Counter\nimport nltk\nfrom nltk.data import load\n#tagdict = load('help/tagsets/upenn_tagset.pickle')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n#import researchpy as rp\nimport scipy.stats as stats\nfrom sklearn.metrics import confusion_matrix\n\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n\nimport seaborn as sns\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-22T07:19:53.414318Z","iopub.execute_input":"2022-06-22T07:19:53.414753Z","iopub.status.idle":"2022-06-22T07:19:53.430354Z","shell.execute_reply.started":"2022-06-22T07:19:53.414720Z","shell.execute_reply":"2022-06-22T07:19:53.428882Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# **EXTRACTION FROM ALIGNED MP-TIME MODELS & PRE-PROCESSING**","metadata":{}},{"cell_type":"code","source":"%%time\n# Load 24 Word2Vec models of MPs in T1 & T2\n\ndictOfModels = {}\nfolderPath = '/kaggle/input/aligned24mptimemodels/kaggle/working/24-aligned-models-by-mp-and-time'\n\n\nfor file in os.listdir(folderPath):\n    filePath = folderPath + '/' + file\n\n    #To accommodate errors while picking up corresponding numpy files of gensim models\n    if(len(filePath.split('.'))>1):\n        continue\n    else:\n        model = gensim.models.Word2Vec.load(filePath)\n        dictOfModels[file] = model\n        \ndictOfModels","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:56.092678Z","iopub.execute_input":"2022-06-22T07:19:56.093268Z","iopub.status.idle":"2022-06-22T07:19:57.014056Z","shell.execute_reply.started":"2022-06-22T07:19:56.093214Z","shell.execute_reply":"2022-06-22T07:19:57.012593Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%%time\n# Extract time, party, MP ID information from name and store in Dataframes\nbrexitEmbeddings = {}\n\nfor k in dictOfModels.keys():\n            \n    time = k.split('df_')[1].split('_')[0]\n    mpId = k.split('df_')[1].split('_')[1]\n    party = k.split('df_')[1].split('_')[2]\n    \n    brexEmbDf =pd.DataFrame()\n    brexEmbDf['model'] = [dictOfModels[k]]\n    brexEmbDf['modelKey'] = k\n    brexEmbDf['time'] = time\n    brexEmbDf['mpId'] = mpId\n    brexEmbDf['party'] = party\n\n    brexitEmbeddings[k] = brexEmbDf\n    \n\nbrexitDf = pd.DataFrame()\n\nfor v in brexitEmbeddings.values():\n    brexitDf = brexitDf.append(v)\n\nbrexitDf.reset_index(inplace=True)\nbrexitDf","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:57.016486Z","iopub.execute_input":"2022-06-22T07:19:57.016881Z","iopub.status.idle":"2022-06-22T07:19:57.132882Z","shell.execute_reply.started":"2022-06-22T07:19:57.016849Z","shell.execute_reply":"2022-06-22T07:19:57.131661Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"# **Calculating Cosine & Modified Similarity**","metadata":{}},{"cell_type":"code","source":"# Slightly modified to now find the cosine difference between provided vectors instead of\n# fetching vectors from known models \n\ndef cosine_similarity(vec1, vec2):\n  sc = 1-spatial.distance.cosine(vec1, vec2)\n  return sc\n\n# Modified Similarity working upon a DF of records comprising 2 vectors (2 MP-time records in a row)\ndef calc_similarity(df,x):\n    #sim(U,V) = ( x )*cosine(U,V) + (1-x)*same_party(U,V)\n    df['similarity']=df['cossims']*x + (1-x)*(df['sameParty'])\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:57.136404Z","iopub.execute_input":"2022-06-22T07:19:57.136732Z","iopub.status.idle":"2022-06-22T07:19:57.144372Z","shell.execute_reply.started":"2022-06-22T07:19:57.136702Z","shell.execute_reply":"2022-06-22T07:19:57.142786Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"'''%%time\n#word = 'brexit'\n\ndef calculate_similarities(change):\n    \n    simFactor = 0.8\n    words = []\n    cosineSimilarities = []\n    medianSimilarities = []\n    meanSimilarities = []\n    stdSimilarities = []\n    \n    for word in change:\n\n        count =0\n        keys=list(brexitEmbeddings.keys())\n\n\n        if word in brexitEmbeddings[keys[0]]['model'][0].wv.index_to_key:\n\n            simMat = pd.DataFrame()\n\n            parties1 = []\n            times1 = []\n            mps1 = []\n            parties2 = []\n            times2 = []\n            mps2 = []\n            cossims= []\n\n            for i, k in enumerate(keys):\n\n                    constKey = brexitEmbeddings[k]\n                    constModel = brexitEmbeddings[k]['model'][0]\n                    constParty = brexitEmbeddings[k]['party'][0]\n                    constTime = brexitEmbeddings[k]['time'][0]\n                    constMP = brexitEmbeddings[k]['mpId'][0]\n\n\n                    for j in range(0,len(keys)):\n\n                        count = count+1\n                        nextVec=brexitEmbeddings[keys[j]]\n                        nextVecModel = nextVec['model'][0]\n                        nextVecParty = nextVec['party'][0]\n                        nextVecTime = nextVec['time'][0]\n                        nextVecMP = nextVec['mpId'][0]\n\n                        cossim = cosine_similarity(constModel.wv[word], nextVecModel.wv[word])\n\n                        parties1.append(constParty)\n                        parties2.append(nextVecParty)\n                        times1.append(constTime)\n                        times2.append(nextVecTime)\n                        mps1.append(constMP)\n                        mps2.append(nextVecMP)\n                        cossims.append(cossim)\n\n            simMat['parties1']=parties1\n            simMat['times1'] = times1\n            simMat['mps1'] = mps1\n            simMat['parties2']=parties2\n            simMat['times2'] = times2\n            simMat['mps2'] = mps2\n            simMat['cossims'] = cossims\n\n\n            #Adding Same party factor as a column\n            simMat['sameParty']=0\n\n            simMat.loc[simMat['parties1']==simMat['parties2'],'sameParty']=1\n            \n            #print('MP-time combinations DFs shape is ',simMat.shape)\n\n            # Calculating Modified similarity\n            similarityDF = calc_similarity(simMat,simFactor)\n\n            medianSimilarity = similarityDF['similarity'].median()\n            medianCosineSimilarity = similarityDF['cossims'].median()\n            \n            meanSimilarity = similarityDF['similarity'].mean()\n            stdSimilarity = similarityDF['similarity'].std()\n\n            \n            print('The median-similarity for ',word, 'is ', medianSimilarity)\n            \n            words.append(word)\n            medianSimilarities.append(medianSimilarity)\n            cosineSimilarities.append(medianCosineSimilarity)\n            meanSimilarities.append(meanSimilarity)\n            stdSimilarities.append(stdSimilarity)\n            \n            \n        else:\n            print('Word', word, 'not found in the vocab of models')\n            \n    medianSimilarityDf = pd.DataFrame({'Word':words, 'median_similarity':medianSimilarities, 'median_cosineSimilarity':cosineSimilarities, 'meanSimilarity':meanSimilarities, 'stdSimilarity':stdSimilarities})\n    return medianSimilarityDf\n'''","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:57.146851Z","iopub.execute_input":"2022-06-22T07:19:57.147185Z","iopub.status.idle":"2022-06-22T07:19:57.158890Z","shell.execute_reply.started":"2022-06-22T07:19:57.147156Z","shell.execute_reply":"2022-06-22T07:19:57.157995Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\n#word = 'brexit'\n\ndef calculate_similarities(change):\n    \n    simFactor = 0.8\n    words = []\n    cosineSimilarities = []\n    medianSimilarities = []\n    meanSimilarities = []\n    stdSimilarities = []\n    \n    for word in change:\n\n        count =0\n        keys=list(brexitEmbeddings.keys())\n\n\n        if word in brexitEmbeddings[keys[0]]['model'][0].wv.index_to_key:\n\n            simMat = pd.DataFrame()\n\n            parties1 = []\n            times1 = []\n            mps1 = []\n            parties2 = []\n            times2 = []\n            mps2 = []\n            cossims= []\n\n            for i, k in enumerate(keys):\n\n                    constKey = brexitEmbeddings[k]\n                    constModel = brexitEmbeddings[k]['model'][0]\n                    constParty = brexitEmbeddings[k]['party'][0]\n                    constTime = brexitEmbeddings[k]['time'][0]\n                    constMP = brexitEmbeddings[k]['mpId'][0]\n\n\n                    for j in range(0,len(keys)):\n\n                        count = count+1\n                        nextVec=brexitEmbeddings[keys[j]]\n                        nextVecModel = nextVec['model'][0]\n                        nextVecParty = nextVec['party'][0]\n                        nextVecTime = nextVec['time'][0]\n                        nextVecMP = nextVec['mpId'][0]\n\n                        cossim = cosine_similarity(constModel.wv[word], nextVecModel.wv[word])\n\n                        parties1.append(constParty)\n                        parties2.append(nextVecParty)\n                        times1.append(constTime)\n                        times2.append(nextVecTime)\n                        mps1.append(constMP)\n                        mps2.append(nextVecMP)\n                        cossims.append(cossim)\n\n            simMat['parties1']=parties1\n            simMat['times1'] = times1\n            simMat['mps1'] = mps1\n            simMat['parties2']=parties2\n            simMat['times2'] = times2\n            simMat['mps2'] = mps2\n            simMat['cossims'] = cossims\n\n\n            #Adding Same party factor as a column\n            simMat['sameParty']=0\n\n            simMat.loc[simMat['parties1']==simMat['parties2'],'sameParty']=1\n            \n            #print('MP-time combinations DFs shape is ',simMat.shape)\n            print(simMat)\n            # Calculating Modified similarity\n            similarityDF = calc_similarity(simMat,simFactor)\n\n            medianSimilarity = similarityDF['similarity'].median()\n            medianCosineSimilarity = similarityDF['cossims'].median()\n            \n            meanSimilarity = similarityDF['similarity'].mean()\n            stdSimilarity = similarityDF['similarity'].std()\n\n            \n            #print('The median-similarity for ',word, 'is ', medianSimilarity)\n            \n            words.append(word)\n            medianSimilarities.append(medianSimilarity)\n            cosineSimilarities.append(medianCosineSimilarity)\n            meanSimilarities.append(meanSimilarity)\n            stdSimilarities.append(stdSimilarity)\n            \n            \n        else:\n            print('Word', word, 'not found in the vocab of models')\n            \n    medianSimilarityDf = pd.DataFrame({'Word':words, 'median_similarity':medianSimilarities, 'median_cosineSimilarity':cosineSimilarities, 'meanSimilarity':meanSimilarities, 'stdSimilarity':stdSimilarities})\n    return medianSimilarityDf\n","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:57.160568Z","iopub.execute_input":"2022-06-22T07:19:57.161212Z","iopub.status.idle":"2022-06-22T07:19:57.190540Z","shell.execute_reply.started":"2022-06-22T07:19:57.161176Z","shell.execute_reply":"2022-06-22T07:19:57.188230Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"%%time\n#word = 'brexit'\n\ndef calculate_similarities(change,antiPartyFactor):\n    \n    words = []\n    cosineSimilarities = []\n    medianSimilarities = []\n    meanSimilarities = []\n    stdSimilarities = []\n    \n    keys = list(brexitEmbeddings.keys())\n\n    keyst1=[kt1 for kt1 in keys if 't1' in kt1]\n    keyst2=[kt2 for kt2 in keys if 't2' in kt2]\n    #print(len(keyst1), len(keyst2))\n\n    \n    for word in change:\n\n        count =0\n        \n        if word in brexitEmbeddings[keys[0]]['model'][0].wv.index_to_key:\n\n            simMat = pd.DataFrame()\n\n            parties1 = []\n            times1 = []\n            mps1 = []\n            parties2 = []\n            times2 = []\n            mps2 = []\n            cossims= []\n\n            for i, k in enumerate(keyst1):\n\n                    constKey = brexitEmbeddings[k]\n                    constModel = brexitEmbeddings[k]['model'][0]\n                    constParty = brexitEmbeddings[k]['party'][0]\n                    constTime = brexitEmbeddings[k]['time'][0]\n                    constMP = brexitEmbeddings[k]['mpId'][0]\n\n\n                    for j in range(0,len(keyst2)):\n\n                        count = count+1\n                        nextVec=brexitEmbeddings[keyst2[j]]\n                        nextVecModel = nextVec['model'][0]\n                        nextVecParty = nextVec['party'][0]\n                        nextVecTime = nextVec['time'][0]\n                        nextVecMP = nextVec['mpId'][0]\n\n                        cossim = cosine_similarity(constModel.wv[word], nextVecModel.wv[word])\n\n                        parties1.append(constParty)\n                        parties2.append(nextVecParty)\n                        times1.append(constTime)\n                        times2.append(nextVecTime)\n                        mps1.append(constMP)\n                        mps2.append(nextVecMP)\n                        cossims.append(cossim)\n\n            simMat['parties1']=parties1\n            simMat['times1'] = times1\n            simMat['mps1'] = mps1\n            simMat['parties2']=parties2\n            simMat['times2'] = times2\n            simMat['mps2'] = mps2\n            simMat['cossims'] = cossims\n\n\n            #Adding Same party factor as a column\n            simMat['sameParty']=0\n\n            simMat.loc[simMat['parties1']==simMat['parties2'],'sameParty']=1\n            \n            #print('MP-time combinations DFs shape is ',simMat.shape)\n            #print(simMat.shape)\n            #print(simMat)\n\n            # Calculating Modified similarity\n            similarityDF = calc_similarity(simMat,antiPartyFactor)\n\n            medianSimilarity = similarityDF['similarity'].median()\n            medianCosineSimilarity = similarityDF['cossims'].median()\n            \n            meanSimilarity = similarityDF['similarity'].mean()\n            stdSimilarity = similarityDF['similarity'].std()\n\n            \n            #print('The median-similarity for ',word, 'is ', medianSimilarity)\n            \n            words.append(word)\n            medianSimilarities.append(medianSimilarity)\n            cosineSimilarities.append(medianCosineSimilarity)\n            meanSimilarities.append(meanSimilarity)\n            stdSimilarities.append(stdSimilarity)\n            \n            \n        else:\n            print('Word', word, 'not found in the vocab of models')\n            \n    medianSimilarityDf = pd.DataFrame({'Word':words, 'median_similarity':medianSimilarities, 'median_cosineSimilarity':cosineSimilarities, 'meanSimilarity':meanSimilarities, 'stdSimilarity':stdSimilarities})\n    return medianSimilarityDf\n","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:57.331459Z","iopub.execute_input":"2022-06-22T07:19:57.332297Z","iopub.status.idle":"2022-06-22T07:19:57.349310Z","shell.execute_reply.started":"2022-06-22T07:19:57.332244Z","shell.execute_reply":"2022-06-22T07:19:57.348042Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"change = ['exiting', 'seaborne', 'eurotunnel', 'withdrawal', 'departures', 'unicorn', 'remainers', 'exit', 'surrender',\n          'departure', 'triggering', 'stockpiling', 'expulsion', 'blindfold', 'cliff', 'lighter', 'exits', 'triggered',\n          'brexiteer', 'soft', 'plus', 'trigger', 'backroom', 'invoked', 'protesting', 'brexit', 'edge', 'canary', \n          'unicorns', 'withdrawing', 'invoking', 'withdrawn', 'manor', 'brexiteers', 'fanatics', 'postponement', \n          'currencies', 'currency', 'operability', 'operable', 'leavers', 'invoke', 'article', 'eurozone', 'clueless',\n          'surrendered', 'cake', 'red', 'euroscepticism', 'prorogation', 'lining', 'gove', 'norway', 'deflationary',\n          'moribund', 'eurosceptic', 'deutschmark', 'courting', 'deal', 'withdraw', 'dab', 'withdrawals', 'eurosceptics',\n          'surrendering', 'aldous', 'lanarkshire', 'leaving', 'signifying', 'roofs', 'ceded', 'absentia', 'treachery',\n          'dollar', 'canada', 'pragmatist', 'oven', 'ready', 'brexiters', 'control', 'capitulation', 'leave', 'referendum',\n          'agreement', 'prorogue', 'smoothest', 'depreciate', 'managed', 'mutiny', 'overvalued', 'ideologues', 'foreign',\n          'eec', 'war', 'prorogued', 'hannan', 'appease', 'pendolino', 'southbound', 'left', 'line', 'hard', 'bill']\n \n    \n#from nltk.stem import WordNetLemmatizer\n #assumes as nouns\n #pos tagging, mapping needed \n\n\nno_change = ['prime', 'even', 'parliament', 'care', 'well', 'constituency', 'tax', 'children',\n             'business', 'report', 'case', 'sure', 'like', 'see', 'state', 'order', 'back', 'new', 'hope', 'local',\n             'secretary', 'public', 'right', 'much', 'say', 'first', 'minister', 'look', 'system', 'whether', \n             'members', 'million', 'good', 'today', 'services', 'clear', 'help', 'time', 'place', 'put', 'last', 'must', 'money', 'one', \n             'way', 'work', 'would', 'think', 'two', 'great', 'could', 'lady', 'us', 'come', 'however', 'may', 'going', 'go',\n             'given', 'year', 'might', 'part', 'get', 'make', 'point', 'committee', 'years', 'also', 'know',\n             'government', 'take', 'house', 'agree', 'member', 'number', 'across', 'made', 'give', 'gentleman', 'important', 'said',\n             'people', 'issue', 'support', 'ensure']\n\n\n\nkeys=list(brexitEmbeddings.keys())\nwo_interest = change+no_change\n\n#Filtering for words present in models' vocab\nwo_interest = list(set(wo_interest).intersection(brexitEmbeddings[keys[0]]['model'][0].wv.index_to_key))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:19:57.494218Z","iopub.execute_input":"2022-06-22T07:19:57.494968Z","iopub.status.idle":"2022-06-22T07:19:57.510364Z","shell.execute_reply.started":"2022-06-22T07:19:57.494932Z","shell.execute_reply":"2022-06-22T07:19:57.509362Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef weighting_experiment(x):\n    similarity = calculate_similarities(wo_interest,antiPartyFactor=x)\n\n    similarity.loc[similarity['Word'].isin(change), 'change']='change'\n    similarity.loc[similarity['Word'].isin(no_change), 'change']='no_change'\n\n    words_of_interest = similarity[similarity['Word'].isin(change+no_change)]\n\n    words_of_interest.loc[words_of_interest['Word'].isin(change), 'semanticDifference'] = 'change' \n    words_of_interest.loc[words_of_interest['Word'].isin(no_change), 'semanticDifference'] = 'no_change'\n\n    #X = words_of_interest['median_similarity'].values.reshape(-1,1)\n    X = words_of_interest[['meanSimilarity','stdSimilarity']]\n    y = words_of_interest['semanticDifference']\n    undersample = RandomUnderSampler(sampling_strategy=1.0)\n\n    X_over, y_over = undersample.fit_resample(X, y)\n    X=X_over\n    y=y_over\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=152)\n\n    #print('Y train values',y_train.value_counts())\n\n    #%%time \n    logreg = LogisticRegression()\n    kf = logreg.fit(X_train, y_train)\n    y_pred = logreg.predict(X_test)\n    #print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n\n    #print('--------------------------x---------------------x------------------------x-----------------------x')\n\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n\n    scoring = {'accuracy' : make_scorer(accuracy_score), \n               'precision' : make_scorer(precision_score,pos_label='change'),\n               'recall' : make_scorer(recall_score,pos_label='change'), \n               'f1_score' : make_scorer(f1_score,pos_label='change')}\n\n    #scores = cross_val_score(kf, X, y, cv=10)\n    scores = cross_validate(kf, X, y, cv=10, scoring=scoring,error_score='raise')\n    \n    '''print('For x = ',x)\n    print('Accuracy', scores['test_accuracy'].mean())\n    print('Precision', scores['test_precision'].mean())\n    print('Recall', scores['test_recall'].mean())\n    print('F1 Score', scores['test_f1_score'].mean())\n'''\n\n    ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n\n    ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n    ax.set_xlabel('\\nPredicted Values')\n    ax.set_ylabel('Actual Values ');\n\n    ## Ticket labels - List must be in alphabetical order\n    ax.xaxis.set_ticklabels(['change','no_change'])\n    ax.yaxis.set_ticklabels(['change','no_change'])\n    \n    ## Display the visualization of the Confusion Matrix.\n    #plt.show()\n    \n    return ({'Accuracy': scores['test_accuracy'].mean(), 'Precision': scores['test_precision'].mean(), 'Recall': scores['test_recall'].mean(), 'F1 Score': scores['test_f1_score'].mean()})\n","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:21:17.567131Z","iopub.execute_input":"2022-06-22T07:21:17.567512Z","iopub.status.idle":"2022-06-22T07:21:17.584400Z","shell.execute_reply.started":"2022-06-22T07:21:17.567482Z","shell.execute_reply":"2022-06-22T07:21:17.582570Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"resultsDF = pd.DataFrame()\nantiPartyFactors = [0, 0.25,0.5,0.75,1] \nfor x in antiPartyFactors:\n    resultMetrics = weighting_experiment(x)\n    resultMetrics = pd.DataFrame([resultMetrics])\n    resultsDF = pd.concat([resultsDF,resultMetrics],ignore_index=True)\n\nresultsDF.insert(0,'x',antiPartyFactors)\nresultsDF   \n#resultsDF.index = antiPartyFactors","metadata":{"execution":{"iopub.status.busy":"2022-06-22T07:21:17.804325Z","iopub.execute_input":"2022-06-22T07:21:17.804723Z","iopub.status.idle":"2022-06-22T07:21:35.495865Z","shell.execute_reply.started":"2022-06-22T07:21:17.804692Z","shell.execute_reply":"2022-06-22T07:21:35.494663Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}